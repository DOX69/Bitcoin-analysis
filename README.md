# Bitcoin-analysis

Bitcoin-analysis is a comprehensive crypto-financial data analysis platform. Designed to offer institutional-grade insights to retail traders, it combines a robust data pipeline (ingestion, transformation, aggregation) with a modern user interface.

## ðŸ“š Table of Contents / Table des MatiÃ¨res

- [ðŸ‡«ðŸ‡· Version FranÃ§aise](#-version-franÃ§aise)
  - [FonctionnalitÃ©s Principales](#fonctionnalitÃ©s-principales)
  - [Tutoriel : Pour Commencer](#tutoriel--pour-commencer)
  - [Guides Pratiques](#guides-pratiques)
  - [Explication](#explication)
  - [RÃ©fÃ©rence](#rÃ©fÃ©rence)
  - [Roadmap](#roadmap)
- [ðŸ‡ºðŸ‡¸ English Version](#-english-version)
  - [Key Features](#key-features)
  - [Tutorial: Getting Started](#tutorial-getting-started)
  - [How-to Guides](#how-to-guides)
  - [Explanation](#explanation)
  - [Reference](#reference)
  - [Roadmap (English)](#roadmap-english)
- [ðŸ“ž Connect With Me](#-connect-with-me)

---

# ðŸ‡«ðŸ‡· Version FranÃ§aise

Bitcoin-analysis est un projet de pipeline de donnÃ©es pour l'analyse des prix du Bitcoin et d'autres cryptomonnaies. Il ingÃ¨re des donnÃ©es OHLCV (Open, High, Low, Close, Volume) depuis l'API gratuite de Coinbase, les transforme via DBT selon une architecture mÃ©dallion (bronze â†’ silver â†’ gold), et calcule des indicateurs comme le RSI. DÃ©ployÃ© sur Databricks avec CI/CD GitHub Actions. Objectif : fournir une base de donnÃ©es analysÃ©e pour une future application web de visualisation d'indicateurs de marchÃ© Bitcoin.

## FonctionnalitÃ©s Principales

- **Pipeline de DonnÃ©es AutomatisÃ©** : Ingestion quotidienne des donnÃ©es OHLCV via l'API Coinbase et orchestration via Databricks Workflows.
- **Architecture MÃ©dallion (Bronze/Silver/Gold)** : Transformation structurÃ©e des donnÃ©es avec DBT pour garantir qualitÃ© et performance.
- **Calcul d'Indicateurs Techniques** : GÃ©nÃ©ration automatique d'indicateurs comme le RSI, et bientÃ´t le MACD, directement en base de donnÃ©es.
- **Support Multi-Devises** : Analyse des paires BTC/USD, BTC/EUR, ETH/USD, ETH/EUR, ETH/BTC, AAVE/USD.
- **CI/CD IntÃ©grÃ©** : DÃ©ploiement continu via GitHub Actions vers les environnements Databricks.
- **Application Web (En cours)** : Interface React/Next.js immersive pour la visualisation et l'analyse.

## Tutoriel : Pour Commencer

### PrÃ©requis
- Compte Databricks (Ã©dition gratuite disponible)
- Python 3.11+
- uv (gestionnaire de paquets Python)
- Databricks CLI

### Installation
1. Clonez le repo :
   ```bash
   git clone https://github.com/DOX69/Bitcoin-analysis.git
   cd Bitcoin-analysis
   ```

2. Installez uv :
   ```bash
   pip install uv
   ```

3. Compilez les dÃ©pendances :
   ```bash
   uv pip compile pyproject.toml -o requirements.txt
   ```

4. Installez Databricks CLI :
   ```bash
   pip install databricks-cli
   ```

### Configuration Databricks
1. CrÃ©ez un compte Databricks (free tier).
2. GÃ©nÃ©rez un token d'accÃ¨s dans Databricks (User Settings > Developer > Access tokens).
3. Ajoutez les secrets dans GitHub (Settings > Secrets and variables > Actions) :
   - `DATABRICKS_HOST` : URL de votre workspace (ex: https://dbc-xxxxxx.cloud.databricks.com)
   - `DATABRICKS_TOKEN` : Votre token d'accÃ¨s

### Premier dÃ©ploiement
1. Allez dans le dossier dbx_workflow :
   ```bash
   cd dbx_workflow
   ```

2. Validez le bundle :
   ```bash
   databricks bundle validate -t dev -p DEV
   ```

3. DÃ©ployez :
   ```bash
   databricks bundle deploy -t dev -p DEV
   ```

4. Lancez le job principal :
   ```bash
   databricks bundle run -t dev -p DEV master_orchestrator_job
   ```

## Guides Pratiques

### DÃ©velopper et tester en local (DEV)
Pour dÃ©velopper et tester localement sur Databricks DEV :

1. CrÃ©ez un environnement virtuel avec uv :
   ```bash
   uv venv
   source .venv/bin/activate  # Sur Windows : .venv\Scripts\activate
   ```

2. Installez les dÃ©pendances pour les tests locaux :
   ```bash
   uv sync --dev
   ```

3. Assurez-vous d'avoir configurÃ© les secrets GitHub (DATABRICKS_HOST, DATABRICKS_TOKEN) pour le profil DEV.

4. Depuis dbx_workflow :
   ```bash
   cd dbx_workflow
   ```

5. Validez, DÃ©ployez et Lancez sur DEV (comme ci-dessus).

Les ressources sont prÃ©fixÃ©es avec '[dev your_username]' et les jobs sont pausÃ©s par dÃ©faut.

### DÃ©ployer en Production (PROD)
Pour dÃ©ployer en production avec jobs actifs :

1. Changez la target vers prod :
   ```bash
   databricks bundle validate -t prod -p PROD --var="pauseStatus=UNPAUSED"
   ```

2. DÃ©ployez :
   ```bash
   databricks bundle deploy -t prod -p PROD --var="pauseStatus=UNPAUSED"
   ```

3. Lancez le job en prod :
   ```bash
   databricks bundle run -t prod -p PROD master_orchestrator_job
   ```

En prod, le schÃ©ma est 'prod.bronze' et les jobs sont actifs (UNPAUSED).

## Explication

### Architecture mÃ©dallion dÃ©taillÃ©e
Le projet implÃ©mente une architecture mÃ©dallion pour la gestion des donnÃ©es :

- **Bronze Layer** :
  - Source : API Coinbase gratuite (pas de rate limits, historique complet).
  - DonnÃ©es : OHLCV quotidiennes pour BTC/USD, BTC/EUR, ETH/USD, ETH/EUR, ETH/BTC, AAVE/USD.
  - Stockage : Delta tables dans Databricks Unity Catalog (catalog.dev/schema.bronze ou prod).
  - Ingestion : PySpark job via Databricks bundle, incremental (full pour premiÃ¨re run, puis delta depuis derniÃ¨re date).

- **Silver Layer** :
  - Transformation : DBT models pour nettoyer et structurer (faits quotidiens par crypto).
  - Format : Tables Delta avec partitions temporelles.
  - Macros DBT : create_update_obt_fact_day_crypto pour upsert.

- **Gold Layer** :
  - AgrÃ©gations : ModÃ¨les DBT pour weekly, monthly, quarterly, yearly aggregations.
  - Indicateurs : RSI calculÃ© via macros DBT (pÃ©riode 14 jours).
  - Analyses : Queries DBT pour explorer rÃ©sultats.

### Choix technologiques
- **DBT** : ELT framework pour transformations SQL, macros pour logique rÃ©utilisable, analyses pour exploration.
- **Databricks** : Plateforme cloud pour exÃ©cution PySpark, stockage Delta, jobs orchestrÃ©s, Unity Catalog.
- **Coinbase API** : API gratuite sans limites, endpoint /products/{ticker}/candles pour donnÃ©es historiques.
- **GitHub Actions** : CI/CD pour dÃ©ploiement automatique sur push (main branch).
- **uv** : Gestionnaire de paquets Python rapide pour dÃ©pendances.
- **PySpark** : Traitement distribuÃ© des donnÃ©es sur Databricks.

## RÃ©fÃ©rence

### Commandes Databricks principales
- `databricks bundle validate -t [dev|prod] -p [DEV|PROD] [--var="pauseStatus=UNPAUSED"]` : Valide la syntaxe du bundle.
- `databricks bundle deploy -t [dev|prod] -p [DEV|PROD] [--var="pauseStatus=UNPAUSED"]` : DÃ©ploie le bundle sur Databricks.
- `databricks bundle run -t [dev|prod] -p [DEV|PROD] master_orchestrator_job` : Lance le job principal d'ingestion et transformation.

### Secrets GitHub
- `DATABRICKS_HOST` : URL du workspace Databricks.
- `DATABRICKS_TOKEN` : Token d'accÃ¨s personnel.

### Points d'API Coinbase
- `GET /products/{ticker}-{currency}/candles` : RÃ©cupÃ¨re donnÃ©es OHLCV historiques (Daily).

## Roadmap

### Application Web React JS
Prochaine Ã©tape : dÃ©veloppement d'une application web interactive en React JS pour visualiser les indicateurs Bitcoin.

**FonctionnalitÃ©s ClÃ©s Ã  Venir :**

- **Trading & Portfolio Management** :
  - **Backtesting de StratÃ©gies** : Simulez vos stratÃ©gies sur 5 ans d'historique pour valider vos hypothÃ¨ses avant de trader.
  - **Suivi de Performance en Temps RÃ©el** : P&L dynamique, alertes de prix et rÃ©Ã©quilibrage de portefeuille.

- **Intelligence Artificielle & PrÃ©visions** :
  - **ModÃ¨les PrÃ©dictifs Profonds** : Utilisation de LSTM et Transformers pour anticiper les mouvements de marchÃ© Ã  court terme.
  - **DÃ©tection d'Anomalies** : Alertes automatiques lors de comportements de marchÃ© inhabituels (flash crashes, pompes).

- **Analyse de Sentiment & On-Chain (The "Alpha")** :
  - **Whale Watching** : Suivi en temps rÃ©el des mouvements des "baleines" (comptes > 1000 BTC) pour anticiper les ventes massives.
  - **Crypto Greed & Fear Index 2.0** : AgrÃ©gation en temps rÃ©el du sentiment sur Twitter, Reddit et Google Trends.
  - **MÃ©triques DeFi** : IntÃ©gration des taux d'intÃ©rÃªt AAVE/Compound pour optimiser le yield farming.

- **Correlations Macro-Economiques** :
  - Heatmaps de corrÃ©lation avec le S&P 500, le Gold, et le DXY pour comprendre l'environnement macro.

---

# ðŸ‡ºðŸ‡¸ English Version

Bitcoin-analysis is a data pipeline project for analyzing Bitcoin and other cryptocurrency prices. It ingests OHLCV (Open, High, Low, Close, Volume) data from the free Coinbase API, transforms it via DBT using a medallion architecture (Bronze â†’ Silver â†’ Gold), and calculates indicators like RSI. Deployed on Databricks with CI/CD via GitHub Actions. Goal: provide an analyzed database for a future web application visualizing Bitcoin market indicators.

## Key Features

- **Automated Data Pipeline**: Daily ingestion of OHLCV data via Coinbase API and orchestration via Databricks Workflows.
- **Medallion Architecture (Bronze/Silver/Gold)**: Structured data transformation with DBT to ensure quality and performance.
- **Technical Indicator Calculation**: Automatic generation of indicators like RSI, and soon MACD, directly in the database.
- **Multi-Currency Support**: Analysis of BTC/USD, BTC/EUR, ETH/USD, ETH/EUR, ETH/BTC, AAVE/USD pairs.
- **Integrated CI/CD**: Continuous deployment via GitHub Actions to Databricks environments.
- **Web Application (In Progress)**: Immersive React/Next.js interface for visualization and analysis.

## Tutorial: Getting Started

### Prerequisites
- Databricks Account (free edition available)
- Python 3.11+
- uv (Python package manager)
- Databricks CLI

### Installation
1. Clone the repo:
   ```bash
   git clone https://github.com/DOX69/Bitcoin-analysis.git
   cd Bitcoin-analysis
   ```

2. Install uv:
   ```bash
   pip install uv
   ```

3. Compile dependencies:
   ```bash
   uv pip compile pyproject.toml -o requirements.txt
   ```

4. Install Databricks CLI:
   ```bash
   pip install databricks-cli
   ```

### Databricks Configuration
1. Create a Databricks account (free tier).
2. Generate an access token in Databricks (User Settings > Developer > Access tokens).
3. Add secrets in GitHub (Settings > Secrets and variables > Actions):
   - `DATABRICKS_HOST`: Your workspace URL (e.g., https://dbc-xxxxxx.cloud.databricks.com)
   - `DATABRICKS_TOKEN`: Your access token

### First Deployment
1. Go to the dbx_workflow folder:
   ```bash
   cd dbx_workflow
   ```

2. Validate the bundle:
   ```bash
   databricks bundle validate -t dev -p DEV
   ```

3. Deploy:
   ```bash
   databricks bundle deploy -t dev -p DEV
   ```

4. Run the main job:
   ```bash
   databricks bundle run -t dev -p DEV master_orchestrator_job
   ```

## How-to Guides

### Run in Dev
To develop and test locally on Databricks DEV:

1. Create a virtual environment with uv:
   ```bash
   uv venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

2. Install dependencies for local tests:
   ```bash
   uv sync --dev
   ```

3. Ensure GitHub secrets (DATABRICKS_HOST, DATABRICKS_TOKEN) are configured for the DEV profile.

4. From dbx_workflow:
   ```bash
   cd dbx_workflow
   ```

5. Validate, Deploy, and Run on DEV (as above).

Resources are prefixed with '[dev your_username]' and jobs are paused by default.

### Deploy to Prod
To deploy to production with active jobs:

1. Change target to prod:
   ```bash
   databricks bundle validate -t prod -p PROD --var="pauseStatus=UNPAUSED"
   ```

2. Deploy:
   ```bash
   databricks bundle deploy -t prod -p PROD --var="pauseStatus=UNPAUSED"
   ```

3. Run the job in prod:
   ```bash
   databricks bundle run -t prod -p PROD master_orchestrator_job
   ```

In prod, the schema is 'prod.bronze' and jobs are active (UNPAUSED).

## Explanation

### Detailed Medallion Architecture
The project implements a medallion architecture for data management:

- **Bronze Layer**:
  - Source: Free Coinbase API (no rate limits, full history).
  - Data: Daily OHLCV for BTC/USD, BTC/EUR, ETH/USD, ETH/EUR, ETH/BTC, AAVE/USD.
  - Storage: Delta tables in Databricks Unity Catalog.
  - Ingestion: PySpark job via Databricks bundle, incremental mode.

- **Silver Layer**:
  - Transformation: DBT models to clean and structure (daily facts per crypto).
  - Format: Delta tables with time partitions.

- **Gold Layer**:
  - Aggregations: DBT models for weekly, monthly, quarterly, yearly aggregations.
  - Indicators: RSI calculated via DBT macros (14-day period).

### Tech Choices
- **DBT**: ELT framework for SQL transformations.
- **Databricks**: Cloud platform for PySpark execution and Delta storage.
- **Coinbase API**: Free historical data API.
- **GitHub Actions**: CI/CD for automatic deployment.
- **uv**: Fast Python package manager.

## Reference

### Main Databricks Commands
- `databricks bundle validate`: Validates bundle syntax.
- `databricks bundle deploy`: Deploys bundle to Databricks.
- `databricks bundle run`: Runs the main ingestion and transformation job.

## Roadmap (English)

### React JS Web App
Next step: development of an interactive React JS web application to visualize Bitcoin indicators.

**Key Upcoming Features:**

- **Trading & Portfolio Management**:
  - **Strategy Backtesting**: Simulate strategies on 5 years of history.
  - **Real-time Performance Tracking**: Dynamic P&L, price alerts.

- **AI & Forecasts**:
  - **Deep Predictive Models**: LSTM and Transformers for short-term market moves.
  - **Anomaly Detection**: Automatic alerts for unusual market behavior.

- **Sentiment & On-Chain (The "Alpha")**:
  - **Whale Watching**: Real-time tracking of large account movements.
  - **Crypto Greed & Fear Index 2.0**: Real-time sentiment aggregation (Twitter, Reddit).
  - **DeFi Metrics**: Integration of AAVE/Compound interest rates.

- **Macro-Economic Correlations**:
  - Correlation heatmaps with S&P 500, Gold, and DXY.

---

# ðŸ“ž Connect With Me

[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/mickael-rakotoarinivo/)
[![Email](https://img.shields.io/badge/Email-D14836?style=for-the-badge&logo=gmail&logoColor=white)](mailto:mickael.rakotoa@gmail.com)
[![GitHub](https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=github&logoColor=white)](https://github.com/DOX69)

---

## Run Checks (Development)

Run all tests before checking in code:
- **Windows**: `./check-all.bat`
- **Linux/Mac**: `./check-all`
